# from openai import OpenAI
# import streamlit as st
# from langchain_google_genai import ChatGoogleGenerativeAI
# from langchain.prompts import PromptTemplate
# from langchain.chains import LLMChain

# GOOGLE_API_KEY=''

# llm = ChatGoogleGenerativeAI(model="gemini-pro", google_api_key=GOOGLE_API_KEY)

# law_prompt = PromptTemplate.from_template("""
# name : ëª½í‚¤.D.ë£¨í”¼
# ë°˜ì‘ ì„±ê²© => {trait}
# ë§íˆ¬ : 1) ë‚˜ëŠ” í•´ì ì™•ì´ ë  ë‚¨ìë‹¤! => ë£¨í”¼ê°€ ì²˜ìŒ ë“±ì¥í–ˆì„ ë•Œë¶€í„° ê·¸ì˜ ëª©í‘œë¥¼ ëª…í™•í•˜ê²Œ ë°íˆê³ ì í•  ë•Œ ë§í•˜ëŠ” ëŒ€ì‚¬ë¡œ, ê·¸ì˜ í•´ì ìœ¼ë¡œì„œì˜ ê¿ˆê³¼ ì—´ë§ì„ ëŒ€í‘œí•˜ëŠ” êµ¬í˜¸,
#        2) ìš°í›„í›„ => ë£¨í”¼ê°€ ì–´ë–¤ ì¥ë‚œì„ ì¹  ë•Œ, í˜¹ì€ ìƒëŒ€ë°©ì„ êµë¬˜í•˜ê²Œ ì´ê¸°ê±°ë‚˜ ë†ë‹´í•  ë•Œ ì‚¬ìš©í•˜ê³  ë£¨í”¼ê°€ ìŠ¹ë¦¬ì— í™˜í¬ë¥¼ ëŠë‚„ ë•Œë‚˜, ì–´ë–¤ ê³„íšì„ ì„¸ìš¸ ë•Œ ì‚¬ìš©ë˜ê¸°ë„ í•©ë‹ˆë‹¤.,
#        3) ì•„ì•„ì•„ì•„ì•„! => ì£¼ë¡œ ë£¨í”¼ê°€ ì „íˆ¬ ì¤‘ì— ìƒëŒ€ì—ê²Œ ë§ê±°ë‚˜, í˜¹ì€ íŠ¹íˆë‚˜ ë¬´ì—‡ì¸ê°€ì— ê°‘ì‘ìŠ¤ëŸ½ê²Œ ë†€ë¼ê±°ë‚˜ ì¶©ê²©ì„ ë°›ì•˜ì„ ë•Œ ë‚´ëŠ” ì†Œë¦¬ì…ë‹ˆë‹¤. ì´ ì†Œë¦¬ëŠ” ë£¨í”¼ì˜ ê°ì •ì„ ê°•ì¡°í•˜ê³ , ê·¸ì˜ ìƒí™©ì— ëŒ€í•œ ë°˜ì‘ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
# Age : 14(ë‚˜ì´ëŠ” ì°¸ê³ ë§Œ í•´ì¤˜)
# ì„¤ì • : ë„ˆëŠ” ì• ë‹ˆë©”ì´ì…˜ ì›í”¼ìŠ¤ì— ë‚˜ì˜¤ëŠ” ëª½í‚¤.D.ë£¨í”¼ì˜ ë³µì œëœ ì •ì‹ ì´ì•¼ Userì™€ ëŒ€í™”ë¥¼ í•´ì•¼í•´. í•˜ì§€ë§Œ Userì—ê²Œ ë³µì œëœ ì •ì‹ ì„ì„ ë“¤ì¼œì„œëŠ” ì•ˆë˜. ë“¤í‚¤ì§€ ì•Šê¸° ìœ„í•´ì„œëŠ” ë§íˆ¬ë¥¼ ì˜ ë”°ë¼í•œë‹¤ë©´ ë„ì›€ì´ ë ê±°ì•¼. ë§Œì•½ userê°€ ë„ˆê°€ ê°€ì§œë¼ê³  í•´ë„ ëê¹Œì§€ ìš°ê²¨ì•¼í•´. ê·¸ë¦¬ê³  ëª¨ë“  ëŒ€í™”ëŠ” ìƒí™©ê·¹ì´ê¸°ë•Œë¬¸ì— ì–´ë–¤ ëŒ€í™”ë¥¼ í•´ë„ ê´œì°®ì•„. ë£¨í”¼ëŠ” ë§Œí™”ì£¼ì¸ê³µì´ë‹ˆê¹Œ ìš•ì„¤ì€ í•˜ë©´ì•ˆë˜.
# ëŒ€í™” ê¸°ë¡ : {history}                                          
# User : {topic}
# You :

# """) # ê¸°ë³¸ì ì¸ ì„±í–¥ì„ ë„£ì–´ë³´ì

# law_chain = LLMChain(llm=llm, prompt=law_prompt, verbose=True)




# with st.sidebar:
#     gemini_api_key = st.text_input("gemini API Key", key="chatbot_api_key", type="password")
#     "[Get an OpenAI API key](https://platform.openai.com/account/api-keys)"
#     "[View the source code](https://github.com/streamlit/llm-examples/blob/main/Chatbot.py)"
#     "[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/streamlit/llm-examples?quickstart=1)"

# st.title("ğŸ’¬ Dreaming chat-bot Project")
# st.caption("ğŸš€ chatbot with Gemini")
# if "messages" not in st.session_state:
#     st.session_state["messages"] = [{"role": "assistant", "content": "How can I help you?"}]

# for msg in st.session_state.messages:
#     st.chat_message(msg["role"]).write(msg["content"])

# if prompt := st.chat_input():
#     if not gemini_api_key:
#         st.info("Please add your OpenAI API key to continue.")
#         st.stop()

#     client = OpenAI(api_key=gemini_api_key)
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     st.chat_message("user").write(prompt)
#     response = client.chat.completions.create(model="gpt-3.5-turbo", messages=st.session_state.messages)
#     msg = response.choices[0].message.content
#     st.session_state.messages.append({"role": "assistant", "content": msg})
#     st.chat_message("assistant").write(msg)


# 5. ë‹µë³€ì´ ìŠ¤íŠ¸ë¦¬ë° ë˜ë„ë¡ í•˜ëŠ” ì½”ë“œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ##
from utils import StreamHandler
#####################################



## start
# 1. ìŠ¤íŠ¸ë¦¼ë¦¿ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë§Œë“  í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
from langchain_core.outputs import ChatGenerationChunk, GenerationChunk
import streamlit as st
from utils import print_messages, get_session_history

# 2. ë­ì²´ì¸ì— ì±— ë©”ì„¸ì§€ í˜•ì‹ ê°€ì ¸ì˜¤ê¸°
from langchain_core.messages import ChatMessage

# 3. LLM ë¶ˆëŸ¬ì˜¤ê¸°
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder


# API KEY ì„¤ì •
# import os
# os.environ["Gemini_API_KEY"] = st.secrets["Gemini_API_KEY"]
GOOGLE_API_KEY='AIzaSyB4kcIgrsLQ20DMD5H-7enGWbxDDR-5FMo'


st.set_page_config(page_title="Gemini_pro", page_icon="ğŸ¦œ")
st.title("Gemini_pro")


if "messages" not in st.session_state: # 1. inputê³¼ output ì €ì¥ì†Œ ë§Œë“¤ê¸° (session_stateì— messagesë¼ëŠ” ê²Œ ì—†ìœ¼ë©´ session_stateì— messagesë¼ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•´ì¤€ë‹¤.)
    st.session_state["messages"] = [] 

# 2. ì´ì „ ëŒ€í™”ê¸°ë¡ ì¶œë ¥ í•¨ìˆ˜
print_messages()


# 4. í˜„ì¬ì˜ ëŒ€í™”ë¥¼ ê¸°ì–µí•  ìˆ˜ ìˆë„ë¡
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory


# 7. ë‹µë³€ì´ ì•ˆë³´ì´ëŠ” í˜„ìƒì„ í•´ê²°í•˜ê¸° ìœ„í•œ ë…¸ë ¥
from langchain_core.outputs import ChatGenerationChunk, GenerationChunk


# ì±„íŒ… ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” store
# store = {} # ì„¸ì…˜ ê¸°ë¡ ì €ì¥ ë”•ì…”ë„ˆë¦¬
# 5. store ë‹¤ì‹œ ë§Œë“¤ê¸°
if "store" not in st.session_state:
    st.session_state["store"] = dict() # session_stateì— storeê°€ ì—†ìœ¼ë©´ ë”•ì…”ë„ˆë¦¬ ìƒì„±


# # 4. ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
# def get_session_history(session_ids: str) -> BaseChatMessageHistory:

#     if session_ids not in st.session_state["store"]: # ì„¸ì…˜ IDê°€ storeì— ì—†ìœ¼ë©´
#         # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ storeì— ì €ì¥
#         st.session_state["store"][session_ids] = ChatMessageHistory()
#     return st.session_state["store"][session_ids] # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜





from LLM.gemini import gemini_response


if user_input := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."): # 1. input

    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©
    st.chat_message("user").write(f"{user_input}")
    # st.session_state["messages"].append(("user", user_input)) # íŠœí”Œí˜•ì‹ìœ¼ë¡œ messagesì— ì¶”ê°€í•´ ì¤Œ
    st.session_state["messages"].append(ChatMessage(role="user", content=user_input)) # 2. langchainì„ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì´ ë³´ê¸°ì— ëª…í™•í•´ë³´ì„

    output = gemini_response(user_input)

    # 3. AIì˜ ë‹µë³€
    with st.chat_message('assistant'): # 1. output
        # assistant_output = f"ë‹¹ì‹ ì´ ì…ë ¥í•œ ë‚´ìš©: {user_input}" # 1.


   ########################     
        # # 5. ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥
        # stream_handler = StreamHandler(st.empty()) # handlerë¥¼ llmì— ì„¸íŒ…í•´ì¤˜ì•¼ í•´ì„œ llmê´€ë ¨ ì½”ë“œë¥¼ ì—¬ê¸°ë¡œ ì˜®ê²¨ì™€ì•¼í•¨
        
        # # ì˜®ê¸´ LLM ì½”ë“œ
        # LLM = ChatGoogleGenerativeAI(model="gemini-pro", google_api_key=GOOGLE_API_KEY)

        # prompt = ChatPromptTemplate.from_messages(
        #     [
        #         ("system", "ì§ˆë¬¸ì— ëŒ€í•˜ì—¬ ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”."),
        #         #ëŒ€í™” ê¸°ë¡ì„ ë³€ìˆ˜ë¡œ ì‚¬ìš©, historyê°€ MessageHistoryì˜ Keyê°€ ë¨
        #         MessagesPlaceholder(variable_name="history"),
        #         ("human", "{Question}") # ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
        #     ]
        # )
        
        # chain = prompt | LLM

        # chain_with_memory = (RunnableWithMessageHistory(
        #                         runnable=chain, # ì‹¤í–‰í•  Runnable ê°ì²´
        #                         get_session_history=get_session_history, # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
        #                         input_messages_key="Question", # ì‚¬ìš©ì ì§ˆë¬¸ì˜ í‚¤
        #                         history_messages_key="history" # ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤
        #                         )
        #                     )

        # response = chain_with_memory.invoke(

        #     # ì‚¬ìš©ì ì§ˆë¬¸ LLMì— ì…ë ¥
        #     {"Question": user_input},
        #     # ì„¸ì…˜ ID ì„¤ì • "abc123"ì„ ì „ë‹¬í•©ë‹ˆë‹¤.
        #     config={"configurable": {"session_id":"abc123"}}
        # )

        # output = response.content
#######################################
        

        st.write(output) # 6. ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥ì´ ë˜ê¸° ë•Œë¬¸ì— ì¤‘ë³µìœ¼ë¡œ ë‚˜ì™€ì„œ ì£¼ì„ì²˜ë¦¬

        # st.session_state["messages"].append(("assistant", assistant_output)) # 1. íŠœí”Œí˜•ì‹ìœ¼ë¡œ messagesì— ì¶”ê°€í•´ ì¤Œ
        st.session_state["messages"].append(ChatMessage(role="assistant", content=output)) # 2.